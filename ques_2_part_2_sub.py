# -*- coding: utf-8 -*-
"""ques_2_part_2_sub.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZRQp-pEwZrYTwtUNlyTEP-DqJkJCqFrQ
"""

#used to work with arrays
import numpy as np
#used to import dataset
import pandas as pd
# used for plotting the dataset
import matplotlib.pyplot as plt
#used for largest value 
import sys
#used for random values
import random
#for doing inverse  of matrices
from numpy.linalg import inv


# dataset upload and using that url I will make a pandas frame
url="https://drive.google.com/file/d/1tRj3p8LM7sevfZbEyC3xK538gD2g2k8a/view?usp=sharing"
url='https://drive.google.com/uc?id=' + url.split('/')[-2]

# test dataset upload and using that url I will make a pandas frame
url_test="https://drive.google.com/file/d/1ee0owgOv_BbS1GEv2gLmyJej_NWlnJ2I/view?usp=sharing"
url_test='https://drive.google.com/uc?id=' + url_test.split('/')[-2]
# the dataset has 100 features, here I am creating the name of the features
name=[]
for i in range (1,101):
  name.append("X"+str(i))
name.append("y")



#used the dataset url to import dataset
dataframe=pd.read_csv(url,names=name)
n=dataframe.shape[0]
one_array=np.ones(n,dtype=int)
dataframe.insert(0,'X0',one_array)
X=dataframe.iloc[:,:-1].T
y=dataframe.iloc[:,-1]
X=np.array(X,float)
#X dataframe has 2 features and 1000 data frames. “n”  is the number of data points and number of features are stored in “number_of_features”
number_of_features=len(X)
# initialization of the parameter array
w=[]
for i in range(101):
  w.append(0)
w=np.array(w,float)

# finding the parameter with closed form
XX_T=np.matmul(X,X.T)
a=np.matmul(inv(XX_T),X)
W_ml=np.matmul(a,y)


# error function
def error(X,y,w):
  return np.linalg.norm(np.matmul(X.T,w)-y)**2

# gradient descent function , it takes the dataset , the label column, and I am running iterations for a certain number of time
def gradient_descent(X,y,w,max_iter):
  W_array=[]
  steps=[]
  step=1
  steps.append(step)
  W_array.append(w)
  XX_T=np.matmul(X,X.T)
  for i in range(max_iter):
    # declaring the learning rate
    learning_rate = (1/step)
    # gradient step
    gradient=2*np.matmul((XX_T),w)-2*np.matmul(X,y)
    gradient_norm = gradient /np.linalg.norm(gradient)
    next=w-learning_rate*gradient_norm
    # updating the parameters
    w=next
    W_array.append(w)
    step=step+1
    steps.append(step)

  return w,W_array,steps

# max iteration of gradient descent is 1000 , I am running the gradient descent algorithm
max_iter=1000
w,W_array,steps=gradient_descent(X,y,w,max_iter)
W_array=np.array(W_array,float).T


# W_array will store the function to be plotted in its diagonals
for i in range(len(W_array[0])):
  W_array[:,i]=W_array[:,i]-W_ml


plot_array=[]
arr=np.matmul(W_array.T,W_array)
for i in range(len(W_array[0])):
  plot_array.append(arr[i][i])
# doing square root as L2 norm needs square root
np.sqrt(plot_array)
plt.figure(figsize=(8,8))
plot_array=np.array(plot_array,float)
steps=np.array(steps)
plt.plot(steps,plot_array)
plt.title("Error vs iteration")
plt.xlabel("iteration number")
plt.ylabel("error")
plt.show()




# TEst dataset load into dataframe and creating X_TEST and Y_TEST
dataframe_test=pd.read_csv(url_test,names=name)
n_TEST=dataframe_test.shape[0]
one_array=np.ones(n_TEST,dtype=int)
dataframe_test.insert(0,'X0',one_array)
X_TEST=dataframe_test.iloc[:,:-1].T
Y_TEST=dataframe_test.iloc[:,-1]
X_TEST=np.array(X_TEST,float)



print("analytical error on train data")
print(error(X,y,W_ml))


print("error (on train data) got when gradient descent is used")
print(error(X,y,w))
print("error on test data")
print(error(X_TEST,Y_TEST,w))