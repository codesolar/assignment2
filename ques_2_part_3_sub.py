# -*- coding: utf-8 -*-
"""ques_2_part_3_sub.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OX5c8D1IpBO65Cw5sMbNoXyY5T6ymMbE
"""

#used to work with arrays
import numpy as np
#used to import dataset
import pandas as pd
# used for plotting the dataset
import matplotlib.pyplot as plt
#used for largest value 
import sys
#used for random values
import random
#for doing inverse  of matrices
from numpy.linalg import inv

# dataset upload and using that url I will make a pandas frame
url="https://drive.google.com/file/d/1tRj3p8LM7sevfZbEyC3xK538gD2g2k8a/view?usp=sharing"
url='https://drive.google.com/uc?id=' + url.split('/')[-2]


# the dataset has 100 features, here I am creating the name of the features
name=[]
for i in range (1,101):
  name.append("X"+str(i))
name.append("y")




#used the dataset url to import dataset
dataframe=pd.read_csv(url,names=name)
n=dataframe.shape[0]
one_array=np.ones(n,dtype=int)
dataframe.insert(0,'X0',one_array)
X=dataframe.iloc[:,:-1].T
y=dataframe.iloc[:,-1]
X=np.array(X,float)
#X dataframe has 2 features and 1000 data frames. “n”  is the number of data points and number of features are stored in “number_of_features”
number_of_features=len(X)

# initialization of the parameter array
w=[]
for i in range(number_of_features):
  w.append(0)
w=np.array(w,float)


# finding the parameter with closed form
XX_T=np.matmul(X,X.T)
a=np.matmul(inv(XX_T),X)
W_ml=np.matmul(a,y)


# error function
def error(X,y,w):
  return np.linalg.norm(np.matmul(X.T,w)-y)**2

#stochastic gradient descent function , it takes the dataset , the label column, and I am running iterations for a certain number of time
def stochastic_gradient_descent(X,y,w,max_iter):
  W_array=[]
  steps=[]
  plot_array=[]
  n=len(X[0])
  for j in range(1,max_iter+1):
    step=j
    index=np.random.randint(n, size=100)
    data= X[: , index]
    out= y[index]
    XX_T=np.matmul(data,data.T)
    Xy=np.matmul(data,out)
    # declaring the learning rate
    learning_rate=1/step 
    # gradient step
    gradient=2*np.matmul((XX_T),w)-2*Xy
    gradient=gradient/np.linalg.norm(gradient)
    # updating the parameters
    w=w-learning_rate*gradient
    steps.append(j)
    W_array.append(w)
    if j % 8 ==0:
      plot_array.append(w)
  return w,W_array,plot_array ,steps


# max iteration of gradient descent is 1000 , I am running the gradient descent algorithm
max_iter=80000
c,W_array,plot_array,steps=stochastic_gradient_descent(X,y,w,max_iter)
w=c
# W_array will store the function to be plotted in its diagonals
W_array=np.array(W_array,float).T


plot_array=np.array(plot_array,float).T

w=np.sum(W_array,axis=1)/max_iter


for i in range(len(plot_array[0])):
  plot_array[:,i]=plot_array[:,i]-W_ml

arr=np.matmul(plot_array.T,plot_array)
plot_arr=[]
for i in range(len(plot_array[0])):
  plot_arr.append(arr[i][i])


step_arr=[]
for i in range(80000):
  if i % 8 == 0:
    step_arr.append(i)

# plotting the function given in the question
plt.figure(figsize=(8,8))
plot_arr=np.array(plot_arr,float)
steps=np.array(steps)
plt.plot(step_arr,plot_arr)
plt.title("Error vs iteration")
plt.xlabel("iteration number")
plt.ylabel("error")
plt.show()

print("analytical error")
print(error(X,y,W_ml))


print("error got when gradient descent is used")
print(error(X,y,w))
# test dataset upload and using that url I will make a pandas frame
url_test="https://drive.google.com/file/d/1ee0owgOv_BbS1GEv2gLmyJej_NWlnJ2I/view?usp=sharing"
url_test='https://drive.google.com/uc?id=' + url_test.split('/')[-2]
# TEst dataset load into dataframe and creating X_TEST and Y_TEST
dataframe_test=pd.read_csv(url_test,names=name)
n_TEST=dataframe_test.shape[0]
one_array=np.ones(n_TEST,dtype=int)
dataframe_test.insert(0,'X0',one_array)
X_TEST=dataframe_test.iloc[:,:-1].T
Y_TEST=dataframe_test.iloc[:,-1]
X_TEST=np.array(X_TEST,float)


print("error on test data")
print(error(X_TEST,Y_TEST,w))

